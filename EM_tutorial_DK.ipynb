{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import cortex\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from Gallant_lab_func.utils import generate_leave_one_run_out\n",
    "from Gallant_lab_func.delayer import Delayer\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import zscore, pearsonr, ttest_1samp, spearmanr, kendalltau, ttest_rel\n",
    "from nltools.stats import isc, fdr, fisher_r_to_z\n",
    "from nltools.external import hrf\n",
    "from nilearn.glm import threshold_stats_img\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from himalaya.kernel_ridge import KernelRidgeCV, MultipleKernelRidgeCV, ColumnKernelizer, Kernelizer\n",
    "from himalaya.viz import plot_alphas_diagnostic\n",
    "from himalaya.backend import set_backend\n",
    "from himalaya.ridge import RidgeCV, GroupRidgeCV, ColumnTransformerNoStack\n",
    "from himalaya.scoring import r2_score_split\n",
    "\n",
    "result_folder = '/media/dasom/caee0336-77a9-438c-af2f-7cf5b88293e7/dasom/SM_DATA/social_movie1/results'\n",
    "\n",
    "mpl.rcParams['axes.linewidth'] = 2\n",
    "mpl.rcParams['xtick.major.width'] = 2\n",
    "mpl.rcParams['ytick.major.width'] = 2\n",
    "mpl.rcParams['xtick.major.size'] = 5\n",
    "mpl.rcParams['ytick.major.size'] = 5\n",
    "mpl.rcParams['axes.spines.top'] = True\n",
    "mpl.rcParams['axes.spines.right'] = True\n",
    "mpl.rcParams['axes.spines.bottom'] = True\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986 + 914 + 666 = 2566\n"
     ]
    }
   ],
   "source": [
    "excluded_tr_ep1 = np.concatenate([np.arange(10)])\n",
    "excluded_tr_ep2 = np.concatenate([np.arange(44)])\n",
    "excluded_tr_ep3 = np.concatenate([np.arange(44)])\n",
    "\n",
    "rest_tr_ep1 = np.delete(np.arange(996), excluded_tr_ep1)\n",
    "rest_tr_ep2 = np.delete(np.arange(958), excluded_tr_ep2)\n",
    "rest_tr_ep3 = np.delete(np.arange(710), excluded_tr_ep3)\n",
    "rest_total = rest_tr_ep1.shape[0] + rest_tr_ep2.shape[0] + rest_tr_ep3.shape[0]\n",
    "rest_tr = np.hstack([rest_tr_ep1, rest_tr_ep2+996, rest_tr_ep3+996+958])\n",
    "print(f'{rest_tr_ep1.shape[0]} + {rest_tr_ep2.shape[0]} + {rest_tr_ep3.shape[0]} = {rest_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# moten_features = []\n",
    "# for r in range(3):\n",
    "#     moten_feature = np.load(f'/home/dasom/SM_DATA/videodata/social_movie1/moten_features/moten_run_{r+1}.npy')\n",
    "#     moten_features.append(moten_feature)\n",
    "# moten_features = np.vstack([moten_features[r] for r in range(3)])\n",
    "\n",
    "features0 = np.load(f'{result_folder}/MODEL/TR_full.npy', allow_pickle = True)\n",
    "features1 = np.load(f'{result_folder}/MODEL/TR_cooccur.npy', allow_pickle = True)\n",
    "features2 = np.load(f'{result_folder}/MODEL/TR_senti_pos.npy', allow_pickle = True)\n",
    "features3 = np.load(f'{result_folder}/MODEL/TR_senti_neg.npy', allow_pickle = True)\n",
    "\n",
    "features1_gf5 = gaussian_filter1d(features1, 5, axis=0)\n",
    "features2_gf5 = gaussian_filter1d(features2, 5, axis=0)\n",
    "features3_gf5 = gaussian_filter1d(features3, 5, axis=0)\n",
    "\n",
    "features = np.hstack([features1, features2, features3])\n",
    "features = features[rest_tr,:]\n",
    "\n",
    "social_movie_run = np.array([493,493,457,457,333,333]) # 6 folds\n",
    "onsets = np.cumsum(social_movie_run)\n",
    "features_split = np.split(features, onsets, axis=0)\n",
    "\n",
    "brain_data_sc_sm = np.load(f'{result_folder}/fMRI_data/movie_data_ex1_sc_sm.npy', allow_pickle = True)[:,:,rest_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI data shape :  (24, 69831, 2566)\n",
      "feature shape :  (2566, 90)\n"
     ]
    }
   ],
   "source": [
    "print('fMRI data shape : ', brain_data_sc_sm.shape)\n",
    "print('feature shape : ', features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Run banded ridge regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a single feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram', assume_finite=True)\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\") # torch cuda setting\n",
    "\n",
    "torch.manual_seed(2022)\n",
    "np.random.seed(2022) # random seed setting\n",
    "\n",
    "alphas = np.logspace(0, 20, 41) # ridge hyperparameter range\n",
    "\n",
    "n_targets_batch = 10000\n",
    "n_alphas_batch = 5\n",
    "n_targets_batch_refit = 100\n",
    "\n",
    "sub_r_scores = []\n",
    "sub_r2_scores = []\n",
    "weights = []\n",
    "for sub in [3]:\n",
    "    r_scores_cv = []\n",
    "    r2_scores_cv = []\n",
    "    weight = []\n",
    "    brain_split = np.split(brain_data_sc_sm[sub], onsets, axis=1) # 6 folds\n",
    "    for train_idx, test_idx in LeaveOneOut().split(range(len(social_movie_run))): # generating and runing 6 folds\n",
    "\n",
    "        train_run_onsets = np.cumsum([0]+social_movie_run[train_idx].tolist())[:-1]\n",
    "\n",
    "        X_train = np.nan_to_num(np.vstack(([zscore(features_split[i], axis=0, nan_policy='omit') for i in train_idx])))\n",
    "        X_train = np.vstack([np.convolve(X_train[:,i],hrf.glover_hrf(1,1)) for i in range(X_train.shape[1])])[:,:X_train.shape[0]].T # check HRF convolve\n",
    "        X_test = np.nan_to_num(zscore(features_split[test_idx[0]], axis=0, nan_policy='omit'))\n",
    "        X_test = np.vstack([np.convolve(X_test[:,i],hrf.glover_hrf(1,1)) for i in range(X_test.shape[1])])[:,:X_test.shape[0]].T # check shape\n",
    "\n",
    "        y_train = np.hstack(([zscore(brain_split[i], axis=1, nan_policy='omit') for i in train_idx])).T\n",
    "        y_test = zscore(brain_split[test_idx[0]], axis=1, nan_policy='omit').T # check shape\n",
    "        \n",
    "        X_train = X_train.astype(\"float32\") # reducing computing cost\n",
    "        X_test = X_test.astype(\"float32\")\n",
    "        y_train = y_train.astype(\"float32\")\n",
    "        y_test = y_test.astype(\"float32\")\n",
    "\n",
    "        n_samples_train = X_train.shape[0]\n",
    "        cv = generate_leave_one_run_out(n_samples_train, train_run_onsets) # nested cross-validation for obtaining optimal alpha\n",
    "        cv = check_cv(cv)\n",
    "\n",
    "        pipeline = make_pipeline(\n",
    "            StandardScaler(with_mean=True, with_std=False), # z-scoring\n",
    "            Delayer(delays=[0]), # [0,1,2,3,4,5,6,7]\n",
    "            RidgeCV(\n",
    "                Y_in_cpu=True,\n",
    "                alphas=alphas, \n",
    "                cv=cv,\n",
    "                solver_params=dict(n_targets_batch=n_targets_batch, \n",
    "                                   n_alphas_batch=n_alphas_batch,\n",
    "                                   n_targets_batch_refit=n_targets_batch_refit)),\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        pred_y = pipeline.predict(X_test)\n",
    "        r = np.array([fisher_r_to_z(pearsonr(y_test[:,r], pred_y[:,r])[0]) for r in range(y_train.shape[1])]) # pearson correlation\n",
    "        r2 = pipeline.score(X_test, y_test) # r-squared value\n",
    "        r2 = backend.to_numpy(r2)\n",
    "\n",
    "        r_scores_cv.append(r)\n",
    "        r2_scores_cv.append(r2)\n",
    "        weight.append(backend.to_numpy(pipeline.named_steps['ridgecv'].coef_)) # weight for each feature\n",
    "        torch.cuda.empty_cache() # release reusable GPU memory\n",
    "\n",
    "    sub_r_scores.append(np.nanmean(r_scores_cv, axis = 0)) # average model prediction scores across folds\n",
    "    sub_r2_scores.append(np.nanmean(r2_scores_cv, axis = 0))\n",
    "    weights.append(weight)\n",
    "\n",
    "sub_r_scores = np.array(sub_r_scores)\n",
    "sub_r2_scores = np.array(sub_r2_scores)\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights :  (1, 6, 90, 69831)\n",
      "r score :  (1, 69831)\n",
      "r2 score :  (1, 69831)\n"
     ]
    }
   ],
   "source": [
    "print('weights : ', weights.shape)\n",
    "print('r score : ', sub_r_scores.shape)\n",
    "print('r2 score : ', sub_r2_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 132.24 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 131.45 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 134.13 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 134.06 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 144.00 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 143.04 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "set_config(display='diagram', assume_finite=True)\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "\n",
    "torch.manual_seed(2022)\n",
    "np.random.seed(2022) # random seed setting\n",
    "\n",
    "feature_names = [\"co-occur\", \"valence\"] # feature subspaces\n",
    "\n",
    "n_iter = 20\n",
    "alphas = np.logspace(0, 20, 41)\n",
    "\n",
    "n_targets_batch = 10000 # 200\n",
    "n_alphas_batch = 5\n",
    "n_targets_batch_refit = 100 # 200\n",
    "\n",
    "r_scores_cv0 = []\n",
    "r_scores_cv1 = []\n",
    "r_scores_cv2 = []\n",
    "r2_scores_cv0 = []\n",
    "r2_scores_cv1 = []\n",
    "r2_scores_cv2 = []\n",
    "weights = []\n",
    "for sub in [3]:\n",
    "    r_scores_batch0 = []\n",
    "    r_scores_batch1 = []\n",
    "    r_scores_batch2 = []\n",
    "    r2_scores_batch0 = []\n",
    "    r2_scores_batch1 = []\n",
    "    r2_scores_batch2 = []\n",
    "    weight = []\n",
    "\n",
    "    brain_split = np.split(brain_data_sc_sm[sub], onsets, axis=1)\n",
    "\n",
    "    for train_idx, test_idx in LeaveOneOut().split(range(len(social_movie_run))):\n",
    "        train_run_onsets = np.cumsum([0]+social_movie_run[train_idx].tolist())[:-1]\n",
    "        X_train = np.nan_to_num(np.vstack(([zscore(features_split[i], axis=0, nan_policy='omit') for i in train_idx])))\n",
    "        X_train = np.vstack([np.convolve(X_train[:,i],hrf.glover_hrf(1,1)) for i in range(X_train.shape[1])])[:,:X_train.shape[0]].T\n",
    "        X_test = np.nan_to_num(zscore(features_split[test_idx[0]], axis=0, nan_policy='omit'))\n",
    "        X_test = np.vstack([np.convolve(X_test[:,i],hrf.glover_hrf(1,1)) for i in range(X_test.shape[1])])[:,:X_test.shape[0]].T\n",
    "\n",
    "        y_train = np.hstack(([zscore(brain_split[i], axis=1, nan_policy='omit') for i in train_idx])).T\n",
    "        y_test = zscore(brain_split[test_idx[0]], axis=1, nan_policy='omit').T\n",
    "        \n",
    "        X_train = X_train.astype(\"float32\")\n",
    "        X_test = X_test.astype(\"float32\")\n",
    "        y_train = y_train.astype(\"float32\")\n",
    "        y_test = y_test.astype(\"float32\")\n",
    "        \n",
    "        n_features_list = [30, 60] # dividing into two feature subspaces\n",
    "        n_features_list_cumsum = np.cumsum([0] + n_features_list)\n",
    "        \n",
    "        solver = \"random_search\" # algorithm for optimizing the log group scalings deltas over cross-validation\n",
    "\n",
    "        solver_function = GroupRidgeCV.ALL_SOLVERS[solver]\n",
    "\n",
    "        n_samples_train = X_train.shape[0]\n",
    "        cv = generate_leave_one_run_out(n_samples_train, train_run_onsets)\n",
    "        cv = check_cv(cv)\n",
    "\n",
    "        solver_params = dict(n_iter=n_iter, \n",
    "                             alphas=alphas,\n",
    "                             n_targets_batch=n_targets_batch,\n",
    "                             n_alphas_batch=n_alphas_batch,\n",
    "                             n_targets_batch_refit=n_targets_batch_refit)\n",
    "        \n",
    "        mkr_model = GroupRidgeCV(groups=\"input\", solver=solver,\n",
    "                                 solver_params=solver_params, cv=cv) \n",
    "\n",
    "        set_config(display='diagram')\n",
    "\n",
    "        preprocess_pipeline = make_pipeline(\n",
    "            StandardScaler(with_mean=True, with_std=False),\n",
    "            Delayer(delays=[0]),\n",
    "            )\n",
    "\n",
    "        # Find the start and end of each feature space in the concatenated X_train.\n",
    "        start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\n",
    "        slices = [\n",
    "            slice(start, end)\n",
    "            for start, end in zip(start_and_end[:-1], start_and_end[1:])\n",
    "        ]\n",
    "        slices\n",
    "\n",
    "        kernelizers_tuples = [(name, preprocess_pipeline, slice_)\n",
    "                              for name, slice_ in zip(feature_names, slices)] # for each feature subspace\n",
    "        \n",
    "        column_kernelizer = ColumnTransformerNoStack(kernelizers_tuples)\n",
    "        column_kernelizer\n",
    "\n",
    "        pipeline = make_pipeline(\n",
    "                        column_kernelizer,\n",
    "                        mkr_model,\n",
    "                        verbose=False\n",
    "                        )\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        scores = pipeline.score(X_test, y_test)\n",
    "        r2_scores_batch0.append(backend.to_numpy(scores))\n",
    "\n",
    "        y_test_pred_split = pipeline.predict(X_test, split=True) # get prediction scores for each feature subspace\n",
    "        split_scores = r2_score_split(y_test, y_test_pred_split)  \n",
    "        r2_scores_batch1.append(backend.to_numpy(split_scores[0]))\n",
    "        r2_scores_batch2.append(backend.to_numpy(split_scores[1]))\n",
    "        \n",
    "        pred_y = pipeline.predict(X_test)\n",
    "        pred_y = backend.to_numpy(pred_y)\n",
    "        r_scores_batch0.append(np.array([fisher_r_to_z(pearsonr(y_test[:,r], pred_y[:,r])[0]) for r in range(y_train.shape[1])]))\n",
    "\n",
    "        pred_y1 = y_test_pred_split[0]\n",
    "        pred_y1 = backend.to_numpy(pred_y1)\n",
    "        r_scores_batch1.append(np.array([fisher_r_to_z(pearsonr(y_test[:,r], pred_y1[:,r])[0]) for r in range(y_train.shape[1])]))\n",
    "\n",
    "        pred_y2 = y_test_pred_split[1]\n",
    "        pred_y2 = backend.to_numpy(pred_y2)\n",
    "        r_scores_batch2.append(np.array([fisher_r_to_z(pearsonr(y_test[:,r], pred_y2[:,r])[0]) for r in range(y_train.shape[1])]))\n",
    "        \n",
    "        weight.append(backend.to_numpy(pipeline.named_steps['groupridgecv'].coef_))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    r_scores_cv0.append(np.nanmean(r_scores_batch0, axis=0))\n",
    "    r_scores_cv1.append(np.nanmean(r_scores_batch1, axis=0))\n",
    "    r_scores_cv2.append(np.nanmean(r_scores_batch2, axis=0))\n",
    "    r2_scores_cv0.append(np.nanmean(r2_scores_batch0, axis=0))\n",
    "    r2_scores_cv1.append(np.nanmean(r2_scores_batch1, axis=0))\n",
    "    r2_scores_cv2.append(np.nanmean(r2_scores_batch2, axis=0))\n",
    "    weights.append(weight)\n",
    "\n",
    "r_scores_cv0 = np.array(r_scores_cv0)\n",
    "r_scores_cv1 = np.array(r_scores_cv1)\n",
    "r_scores_cv2 = np.array(r_scores_cv2)\n",
    "r2_scores_cv0 = np.array(r2_scores_cv0)\n",
    "r2_scores_cv1 = np.array(r2_scores_cv1)\n",
    "r2_scores_cv2 = np.array(r2_scores_cv2)\n",
    "weights = np.array(weights)\n",
    "weights_mean = np.mean(weights, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huth et al., 2016 // Statistical test for each subject separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.regression.linear_model as sm\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "social_movie_run = np.array([493,493,457,457,333,333])\n",
    "TR_length = np.array([np.sum(social_movie_run[np.delete(np.arange(6), r)]) for r in range(6)])\n",
    "r2_dist = np.sort([np.mean([sm.OLS(zscore(np.random.normal(size = TR_length[r])), zscore(np.random.normal(size = TR_length[r]))).fit().rsquared for r in range(6)]) for rr in range(1000)])\n",
    "r_dist = np.sort([np.mean([pearsonr(np.random.normal(size = TR_length[r]), np.random.normal(size = TR_length[r]))[0] for r in range(6)]) for rr in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r2 = np.array([np.sum(sub_r2_scores[0][v] < r2_dist)/1000 for v in range(69831)])\n",
    "p_r = np.array([np.sum(sub_r_scores[0][v] < r_dist)/1000 for v in range(69831)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdr-corrected p_r2 :  0.005\n",
      "fdr-corrected p_r :  0.024\n"
     ]
    }
   ],
   "source": [
    "print('fdr-corrected p_r2 : ', fdr(p_r2))\n",
    "print('fdr-corrected p_r : ', fdr(p_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kumar et al., 2024 // Group statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample bootstrap hypothesis test\n",
    "def work_func2(voxels):\n",
    "    np.random.seed(2022)\n",
    "\n",
    "    # r_scores_cv0 = np.load(f'{result_folder}/ENCODING/r_value_cv_full_2fs_f90_score0_gf5.npy')\n",
    "\n",
    "    repetition = 5000\n",
    "    rand_dist = []\n",
    "    for v in voxels:\n",
    "        ran_score = []\n",
    "        for r in range(repetition):\n",
    "            ran_score.append(np.mean(np.random.choice(r_scores_cv0[:,v], size=24, replace=True)))\n",
    "        rand_dist.append(ran_score)\n",
    "    return rand_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***run time(sec) : 1402\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "cpu = 10\n",
    "pool = Pool(cpu)\n",
    "\n",
    "start = int(time.time())\n",
    "results = pool.map(work_func2, np.array_split(np.arange(69831), cpu))\n",
    "rand_dist = np.concatenate([results[r] for r in range(cpu)])\n",
    "print(\"***run time(sec) :\", int(time.time()) - start)\n",
    "\n",
    "p0 = np.array([np.sum(rand_dist[:,v] < 0)/5000 for v in range(69831)])\n",
    "th_p0 = fdr(p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = nib.load('./files/MNI152NLin2009cAsym_3mm_mask.nii.gz')\n",
    "brain_mask = nib.load('./files/MNI152NLin2009cAsym_3mm_mask.nii.gz').get_fdata()\n",
    "score_brain = np.zeros((brain_mask.shape))\n",
    "score_brain[np.where(brain_mask > 0)] = t3\n",
    "z_map = nib.Nifti1Image(score_brain, cc.affine, cc.header)\n",
    "\n",
    "clean_map, threshold = threshold_stats_img(z_map, alpha=.05, height_control='fdr', cluster_threshold=0)\n",
    "clean_map_data = clean_map.get_fdata()\n",
    "clean_map_data[clean_map_data <= 0] = 0\n",
    "print(threshold)\n",
    "\n",
    "clean_map_data3 = clean_map_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 14155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<JS: window.viewer>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping server\n",
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "roi_mask = nib.load('./files/BNA_3mm_atlas.nii.gz').get_fdata()\n",
    "brain_mask = nib.load('./files/MNI152NLin2009cAsym_3mm_mask.nii.gz').get_fdata()\n",
    "\n",
    "subj = 0\n",
    "visualized_data = sub_r_scores[subj].copy()\n",
    "visualized_data[p_r > fdr(p_r)] = np.nan\n",
    "pycor_mask = np.full(brain_mask.shape, np.nan)\n",
    "pycor_mask[brain_mask > 0] = visualized_data\n",
    "    \n",
    "vol_data = cortex.Volume(pycor_mask.transpose(2,1,0), 'cvs_avg35_inMNI152', 'social_movie', cmap='plasma',\n",
    "                        vmin = 0, vmax = 0.03)\n",
    "cortex.webshow(vol_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 39755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<JS: window.viewer>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "brain_mask = nib.load('./files/MNI152NLin2009cAsym_3mm_mask.nii.gz').get_fdata()\n",
    "\n",
    "scores_cv1 = np.load(f'{result_folder}/ENCODING/r_value_cv_full_2fs_f90_score1_gf5.npy')\n",
    "scores_cv2 = np.load(f'{result_folder}/ENCODING/r_value_cv_full_2fs_f90_score2_gf5.npy')\n",
    "\n",
    "# visualized_data1 = scores_cv1[3].copy()\n",
    "# visualized_data1[visualized_data1 < 0] = 0\n",
    "# visualized_data2 = scores_cv2[3].copy()\n",
    "# visualized_data2[visualized_data2 < 0] = 0\n",
    "\n",
    "score_brain1 = np.zeros(brain_mask.shape)\n",
    "score_brain1[brain_mask==1] = visualized_data2\n",
    "\n",
    "score_brain2 = np.zeros(brain_mask.shape)\n",
    "score_brain2[brain_mask==1] = visualized_data1\n",
    "\n",
    "\n",
    "vol_data = cortex.Volume2D(score_brain1.transpose(2,1,0), score_brain2.transpose(2,1,0), \"cvs_avg35_inMNI152\", \"social_movie\",\n",
    "                           vmin = 0, vmax = 0.05, vmin2 = 0, vmax2 = 0.05,\n",
    "                           cmap='MaWtCy_covar')\n",
    "                           # cmap='RdYlBu_covar')\n",
    "cortex.webshow(vol_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
